
The task of vision subsystem is to handle the image data acquired from Raspbery Pi camera. This handling includes two basic subtasks: sign detection and sign recognition (classification).

Three basic signs detected and recognized by the system are shown on figure \ref{fig:raw-signs}. They are printed on a flat paper surface.

\begin{figure}[th!]
	\centering
	\begin{subfigure}[b]{0.2\textwidth}
		\centering
		\includegraphics[scale=0.25]{sign-circle.png}
		\subcaption{Circle (goal)}
	\end{subfigure}
	\begin{subfigure}[b]{0.4\textwidth}
		\centering
		\includegraphics[scale=0.25]{sign-arrow.png}
		\subcaption{Arrow (direction change)}
	\end{subfigure}
	\begin{subfigure}[b]{0.2\textwidth}
		\centering
		\includegraphics[scale=0.25]{sign-cross.png}
		\subcaption{Cross (stop)}
	\end{subfigure}
	\caption{Shape of signs used by the system}
	\label{fig:raw-signs}
\end{figure}

The basic workflow of the vision processing module is split into functional blocks, shown on the figure \ref{fig:vision-module-blocks}.

%\begin{figure}[th!]
%	\centering
%		\includegraphics[scale=0.5]{image-module.png}
%	\caption{Workflow of vision module}
%	\label{fig:vision-module-blocks}
%\end{figure}

\begin{figure}[!ht]
	\centering
	\begin{tikzpicture}[node distance = 2cm, auto]
	\tikzstyle{block} = [rectangle, draw, fill=blue!20, 
	text width=5em, text centered, rounded corners, minimum height=4em]
	\tikzstyle{line} = [draw, -latex', line width =2pt]
	% Place nodes
	\node [block] (ci) {Color image};
	\node [block, right of=ci, node distance = 3 cm] (si) {Segmented image};
	\node [block, right of=si, node distance = 3 cm] (os) {Object shape};
	\node [block, right of=os, node distance = 3 cm] (sign) {Sign};
	% Draw edges
	\path [line] (ci) -- (si);
	\path [line] (si) -- (os);
	\path [line] (os) -- (sign);
	\end{tikzpicture}
	\caption{Workflow of vision module}
	\label{fig:vision-module-blocks}
\end{figure}




Different methods and challenges encountered in vision module are described and discussed separately in the following chapters of the report.
\paragraph{Inverse Perspective Mapping}

In this project, a single camera mounted in the front of the robot with small tilting angle is used to capture the images. Since objects of interest are found on a ground plane, before applying any image recognition and classification methods, it is common-sense to process obtained camera images using Inverse Perspective Mapping (IPM).
Knowing that camera tilting angle and position on the robot is fixed, this transform is just a  plane-to-plane homography to convert the perspective image to
top view to the ground plane image. The IPM can be applied to all three planes of colour image separately or only on selected colour plane. Example of the transform can be seen in figure \ref{fig:ipm_example}.
\begin{figure}[!ht]
	\centering
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width = 1\textwidth]{invPersInput.png}
		\caption{Robot captured image}
		\label{fig:}
	\end{subfigure}%
	~ 
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width = 1\textwidth]{invPersOutput.png}
		\caption{Output image after IPM}
		\label{fig:}
	\end{subfigure}
	%main caption
	\caption{Example of Inverse Perspective Mapping on a single colour image plane}
	\label{fig:ipm_example}
\end{figure}

\paragraph{Thresholding}

The first and the most intuitive approach to sign detection procedure was simple color thresholding, since signs which had to be detected are purely red.

The initial choice for color thresholding was HSV color space. Ranges of H (hue), S (saturation) and V (value) channels are [0,180], [0, 255] and [0, 255], respectively. In order to segment only red color, values of H (hue) channel greater than 170 and lower than 10 were used, since this range represents the band of red color. Additionally, too dark (V $ < $ 40), to bright (V $ > $ 220) and non-saturated pixels (S $ < $ 40) were rejected to improve the quality of segmentation. Results are shown on the figure ~\ref{fig:color-spaces}.

\begin{figure}[th!]
	\centering
	\begin{subfigure}[b]{0.3\textwidth}
		\centering
	\includegraphics[scale=0.33]{thresholding-raw.png}
	\subcaption{Raw camera input}
	\end{subfigure}
	\begin{subfigure}[b]{0.3\textwidth}
		\centering
		\includegraphics[scale=0.7]{thresholding-hsv.png}
		\subcaption{HSV color space}
	\end{subfigure}
	\begin{subfigure}[b]{0.3\textwidth}
		\centering
		\includegraphics[scale=0.7]{thresholding-YCrCb.png}
		\subcaption{YCrCb color space}
	\end{subfigure}
	\caption{Comparison of color spaces used for thresholding}
	\label{fig:color-spaces}
\end{figure}

After some testing, YCrCb color space proved to give better results for segmentation, as shown on figure above. As a result, YCrCb colour space was used in final implementation.

Usage of IR camera did not affect the recognition quality of the thresholding procedure in regular, normal lighting conditions. On the other hand, it drastically increased the accuracy in low lighting conditions, which is described in Hardware section of the report.

The biggest connected component (blob) in the thresholded image is considered to be a sign. Bounding box of the biggest blob determined the area of the thresholded image to be passed in as an input to the sign classification module. 

\paragraph{Classification of the signs using statistical moments}

Taking into consideration the relatively low computational power of the hardware platform used to develop the system, priority for the chosen sign classification algorithm was low complexity.

After some thorough analysis of the available option, statistical moments proven to be the best option.

For an arbitrary binary (i.e. thresholded) input image system can compute statistical moments of desired order. The moment of importance in case of sign classification was center of mass or first order moment. It can be thought of average coordinate of all white pixels for both axes.

\begin{figure}[th!]
	\centering
	\begin{subfigure}[b]{0.25\textwidth}
		\centering
		\includegraphics[scale=0.55]{moments-arrow-raw.png}
		\subcaption{Raw camera input}
	\end{subfigure}
	\begin{subfigure}[b]{0.25\textwidth}
		\centering
		\includegraphics[scale=0.8]{moments-arrow-segm.png}
		\subcaption{Arrow}
	\end{subfigure}
	\begin{subfigure}[b]{0.2\textwidth}
		\centering
		\includegraphics[scale=0.8]{moments-cross-segm.png}
		\subcaption{Cross}
	\end{subfigure}
	\begin{subfigure}[b]{0.2\textwidth}
		\centering
		\includegraphics[scale=0.9]{moments-circle-segm.png}
		\subcaption{Circle}
	\end{subfigure}
	\caption{Visual cues for sign classification using statistical moments}
	\label{fig:statistical-moments}
\end{figure}

Figure \ref{fig:statistical-moments} demonstrates intuition behind visual cues used for discrimination of three different types of signs. By computing the distance of center of the mass (green rectangle) and center of the cropped image segment where the sign was detected (blue rectangle) we can perform initial discrimination between \textit{arrow} and \textit{rest two signs}. Distance is going to be close to zero in case of cross and circle since object are symmetrical around two mutually normal axes. Arrow is not symmetrical around one axis, so it's center of the mass will be slightly shifter towards the tip of the arrow and this is the property used to distinguish an arrow from the circle and cross.
Table \ref{tab:moments} shows average distance of center of the mass from image center proportional to cropped image size. Clear observation is that thresholding this distance to 10 $\%$ can give is pretty good estimate of probability that sign belongs to arrow class.

If this distance is close to zero, additional step is needed to determine if the observed sign is cross or circle. Approach used in this step was investigation of zero-order moment, which gives the number of white pixels in the image. By taking the ratio of zero order moment and total number of pixels of sign area, we can distinguish between cross and circle. Intuition behind this metric lies in the fact that circle has bigger surface than cross of the same bounding box size, so the percentage of space it occupies in the cropped image area where the sign is going to be larger than if it was cross. Table \ref{tab:moments} gives suitable value of threshold of 70 $\%$.

\begin{table}[th!]
\centering
	\begin{tabular}{l*{2}{c}r}
		\toprule
		Sign class			& Distance of first moment & Ratio of second order moment  \\
		\hline
		Arrow 				& 18.41 & 56.44  \\
		Cross            	& 1.39 & 54.20  \\
		Circle           	& 0.97 & 89.63  \\
		\bottomrule
	\end{tabular}
\caption{Numerical values of statistical moments \\given in percentage with respect to image size}
\label{tab:moments}
\end{table}

Described method proved to be sufficiently accurate for practical purposes, while still remaining \textit{computationally tractable} on available hardware. More robust classification methods will be discussed in section \ref{sec:future-improvements} and are subject to future investigation.

\paragraph{Neural network classifier}


Neural networks are one of the well known and widely used machine learning technique for solving classification problems. Basic operating principle can be briefly summarized in the next paragraph.

Network consist of input layer, output layer and arbitrary number of levels (often one), as shown on figure \ref{fig:nn-topology}. Input image is converted to vector of values and fed into the input layer, which means that number of neurons in the input layer is equal to the size of the input. Size of the output layer is equal to the number of classes, since each individual neuron in output layer responds to one specific class. Size of hidden layer is chosen in arbitrary way and it has effect on the network performance and accuracy.


\begin{figure}[th!]
	\centering
	\begin{subfigure}[b]{1\textwidth}
		\centering
		\includegraphics[scale=0.6]{nn-topology.png}
		\subcaption{Neural network topology}
		\label{fig:nn-topology}
	\end{subfigure}
	\begin{subfigure}[b]{0.45\textwidth}
		\centering
		\includegraphics[scale=0.5]{nn-sliding-window.png}
		\subcaption{Sliding window concept}
		\label{fig:nn-sliding-window}
	\end{subfigure}
	\begin{subfigure}[b]{0.45\textwidth}
		\centering
		\includegraphics[scale=0.5]{nn-train-data.png}
		\subcaption{Training data sets}
		\label{fig:nn-train-data}
	\end{subfigure}
	\caption{Neural network classifier}
	\label{fig:nn-general}
\end{figure}

Signals in neural network are propagated in two direction: from input layer to the output layer (forward propagation) and from output layer to the input layer (back propagation). Forward propagation is simple weighted sum of all inputs to each of the neuron.

Training of the neural network is necessary process of determining the weights of all of the connections between each pair of neurons so that network is able to classify each of the classes from training data set. This step involves computing partial derivatives of cost function for each neuron with respect to each edge weight, which is very computationally expensive step. Because of this reason final implementation of neural network was not trained on Raspbery Pi, but on more powerful PC. Result of training was a file describing complete neural network connectivity, which was loaded to vision module to perform detection and classification. 

System on the robot was running only forward propagation step, which was not computationally expensive to run. This was working for image containing only sign to be classified and if used on the result of object detection step, which involved image thresholding and segmentation.

In order to avoid non-robust color thresholding step, position of the sign should be determined by the neural network. This is done using sliding window concept, shown on figure \ref{fig:nn-sliding-window}. Using this approach system runs neural network classifier for each sub-image of certain scale and position in the image and determines is there a sign in observed position and, if there is, which one is it. This approach required a lot of checks on a single image so it introduced big latency in processing pipeline, which made it unusable on the actual system. 

Although analyzed and prototyped on the system, neural network can't be used to determine the position of the sign, so segmentation still remains the bottleneck of the robustness. Final implementation didn't use neural network approach in processing pipeline because of described reasons.

\paragraph{Computing arrow orientation}

After sign has been classified as arrow, additional step needs to be performed in order to extract the arrow angle relative to the orientation of the robot at the moment of observation.

Initial idea was to use already computed information about the statistical moments to get the orientation of the arrow. Center of the mass and center of the cropped image segment containing the sign form a vector pointing towards tip of the arrow. We can exploit this information to determine the angle of the vector, as shown on figure \ref{fig:arrow-angle-computation}. Although not so robust, this was intuitive and the fastest method to prototype since all moments are already computed for sign classification step, which meant that this method would reduce computational overhead.

\begin{figure}[th!]
	\centering
	\begin{subfigure}[b]{0.45\textwidth}
		\centering
		\includegraphics[width = 1\textwidth]{moments-arrow-raw.png}
		\subcaption{Raw camera input}
	\end{subfigure}
	\begin{subfigure}[b]{0.45\textwidth}
		\centering
		\includegraphics[width = 1\textwidth]{moments-arrow-segm-angle.png}
		\subcaption{Arrow}
	\end{subfigure}
	~
	\begin{subfigure}[b]{0.45\textwidth}
		\centering
		\includegraphics[width = 1\textwidth]{moments-arrow-segm-angle-math.png}
		\subcaption{Angle from moments}
	\end{subfigure}
	\begin{subfigure}[b]{0.45\textwidth}
		\centering
		\includegraphics[width = 1\textwidth]{moments-arrow-segm-angle-fitting.png}
		\subcaption{Line fitting}
		\label{fig:line-fitting}
	\end{subfigure}
	\caption{Computation of the arrow orientation}
	\label{fig:arrow-angle-computation}
\end{figure}

After implementation and extensive testing of mentioned method, some scenarios where it fails were met. One of them is shown in the figure, where wrong value of the angle is computed or it was not reliable enough (it was oscillating a lot). Oscillation problem could have been solved by filtering, but instead a different approach was chosen.

Instead of focusing on statistical properties of the contour of the arrow better approach is to use the whole information about it's contour. Contour of the arrow shape is extracted from segmented binary image of the arrow. Array of points representing the contour is then fitted to the line in \textit{Linear Least Squares fashion (LLS)}, which is a well known function minimization framework for linear systems.

The basic idea is posing the problem of finding unknown arrow angle $\theta$ as function minimization problem, where input is defined as set $C$ of $N$ contour points $C_i \in \mathbb{R}^2$. Cost function on a set of points $C$ and chosen angle $\theta$ is defined as sum of the squared distance of each contour point $C_i$ from the line with angle $\theta$, which is formally given by equation \ref{eq:LLS}. Minimizing this function over $\theta$ gives the optimal line angle that fits the given contour points.

\begin{equation}
f(C,\theta)=
\frac{1}{N}
\sum_{i=1}^{N}distance(C_i, line(\theta))^2
\label{eq:LLS}
\end{equation}

Figure \ref{fig:line-fitting} shows two arbitrary lines on top of the segmented image of the arrow. Blue line has higher value of the cost function than the red line because it does not fit the given data (contour points) the best possible way. Red line is the optimal angle for given shape, determined by least square solution of the equation \ref{eq:LLS}.

OpenCV has LLS implementation of described line fitting algorithm, which was used in the final version of the solution. Using this method improved the accuracy and stability of orientation of the arrow without significant impact on performance, since optimization problem can be solved using techniques linear algebra, which are computationally inexpensive.

\paragraph{Solving partial occlusion and re-detection}

Problem of partial occlusion arises when robot is moving forward and a sign is slowly getting into the area observed by robot. Some time is needed before the sign completely enters the view and attempt of classification partially visible sign would result in misclassification in most of te cases.

Also, there is a problem of detecting proximity of the robot to the observed sign. The desired behaviour is to switch states only when the robot is close enough to detect the sign and not as soon as detects it.

In order to prevent attempts of classifications of partially visible signs and detecting proximity of the robot with respect to the sign, the whole view of the camera is split into two regions of interest: \textit{perceptive area} and \textit{proximity area} as shown on figure \ref{fig:camera-view-areas}. Using these view areas, the system is able to overcome the described problems in very easy and intuitive way.

\begin{figure}[th!]
	\centering
		\includegraphics[scale=0.8]{thresholding-raw.png}
	\caption{Perceptive (green) and proximity (red) area}
	\label{fig:camera-view-areas}
\end{figure}

Detection of the sign (segmentation and blob detection) is performed at every iteration and position of the object is extracted as center of the detected blob. For predefined camera position and physical size of the signs, it can be assumed that \textit{if object's center is inside the perception area it will be completely inside the camera view}. This allows a reliable determination of the exact classification moment, without any risk of classifying partially visible objects.

Similar strategy is used for detecting the proximity of the sign: if its center is in the proximity area it is assumed to be close to the robot. Since the system faces slightly delayed image processing and low FPS, it will take some time for the robot to determine that the sign was approached. This will result in the robot ending up exactly on the sign at the moment when it switches to execution of the command defined by the sign.
